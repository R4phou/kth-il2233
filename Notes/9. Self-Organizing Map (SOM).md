[IL2233VT24_Lecture_9_slides](IL2233VT24_Lecture_9_slides.pdf)
# 9.1 SOM
## What is SOM?
A #SOM **Self-Organizing Map** is one of the most popular #NN models, but a different type. It is also known as *Kohonen model/map/network*.

It is based on #unsupervised-learning (no labeled data, unknown characteristics of the input data).

It belongs to the category of **competitive learning** (vs error-correction learning such as error back-propagation with gradient descent) **networks**.

#SOM can be used to detect features inherent to the problem and thus has also been called **SOFM - Self-Organizing Feature Map**.

It provides a topology preserving mapping from the high dimensional space to map units or neurons, which usually form a two-dimensional lattice. This means **mapping from high dimensional space onto a 2D plane**.
- The **mapping** *preserves the relative distance between the data points*
- Data points near each other in the input space are mapped nearby neurons in the #SOM. It can thus serve as a #cluster **analyzing tool of high-dimensional data**.

#SOM has the **capability to generalise**.
- The network can recognize or characterize inputs it has never encountered before.
- A new input is assimilated with the neuron to which it is mapped.
## What does it look like?
#SOM is a **two-layer neural network**:
- **Input layer** - number of neurons determined by dimension of the input vector
- **Output layer** - neurons have connections to neighbor neurons, indicating only adjacency of nodes
The two layers are fully connected, meaning a large amount of weights.
	![](Pasted%20image%2020240507114848.png)

If  we have a 1D output layer, it gives us a clearer picture on the input layer and the weight vectors of the output layer.
	![](Pasted%20image%2020240507115007.png)

Typically, the **output layer** is a **2D array of neurons** connected to each other via a rectangular or hexagonal topology. The topological relations are shown by the lines between the neurons.$$M=m_{1},\dots,m_{p\times q}$$One neuron is a weight vector (sometimes calles a codebook vector), which has the same dimension as the input vectors (n-dimensional).$$m_{i}=[m_{i1},\dots,m_{in}]$$The neurons are connected to adjacent neurons by a neighborhood relation, this dictates the topology/structure of the map.![](Pasted%20image%2020240507115704.png)

## SOM Example
Here is SOM example (\_o_/).![](Pasted%20image%2020240507120138.png)
## Topology relation and neighborhood
One can also define a distance between the map units according to their topology relations.

Adjacent neighbors $N_{c}$ belong to the neighborhood of the neuron $M_{c}$. The #neighborhood-function should be a *decreasing function of time* $N_{c} = N_{c}(t)$.

Neighborhoods of different sizes are illustrated here in an hexagonal lattice. In the smallest hexagon, there are all the neighbors belonging to the smallest neighborhood of the neuron.![](Pasted%20image%2020240507120825.png)
# 9.2 Training
#Training is an **iterative process through time**. It requires a lot of computational effort and thus is time-consuming.

The training consists of drawing sample vectors from the input data set and "teaching" them to the #SOM.

The teaching consists of choosing a winner unit, called **best matching unit** #BMU by a similarity measure and updating the values of weight vectors in the neighborhood the #BMU.

This process is repeated a number of times.

The **network training** is composed of various algorithmic steps:
- **Start**: determine a #SOM network $X Ã— Y$ 
- **Initiation**: initialize the network weights
- **Competition**: find #BMU for a given sample 
- **Cooperation**: update weights in the neighbor region of #BMU 
- **Adaptation**: adjust learning rate and neighborhood size 
- **Validation**: test if the #SOM performs well for new data

And of 3 loops:
1. For all input vectors
2. For all iterations
	- One iteration is one #epoch which uses all input vectors once.
	- For each #epoch, update the learning rate and neighborhood size.
3. Repeat from **initialisation** **if** **generalisation performance is not satisfactory**

Schema:
	![](Pasted%20image%2020240507121636.png)
## Initialisation
There are 3 different types of network initialisations:
- #Random-initialisation means assigning **random values to weight vectors**. This is the case if nothing or little is known about the input data at the time of the initialisation.
- #Initial-samples of the input data set can be used for weight vector initialisation. This has the advantage that the points automatically lie in the same part of the input space with the data.
- One initialization method utilizes the **principal component analysis** #PCA of the input data. The weight vectors are initialized to *lie in the same input space that is spanned by two eigenvectors corresponding to the largest eigenvalues of the input data*. This has the effect of **stretching the** #SOM to the **same orientation as the data having the most significant amounts of energy**.
## Competition
In one training step, one sample vector is drawn randomly from the input data set.

This vector is fed to all units in the network and a similarity measure is calculated between the input data sample and all the weight vectors.

The best-matching unit #BMU is **chosen** to *be the weight vector with greatest similarity with the input sample*.
	The *similarity is usually defined by means of a **distance measure**.* For example in the case of #Euclidean-distance, the #BMU is the closest neuron to the sample in the input space.

The #BMU $m_{c}$ is the weight vector that matches a given input vector $x$ best. It is thus defined formally as the neuron which:$$||x-m_{c}||=\min_{i}\{ ||x-m_{i}|| \}$$
##### Euclidean distance
The #Euclidean-distance is defined as such$$d_{E}(x,y)=\lvert \lvert x-y \rvert  \rvert $$ with $||x||=\sqrt{ \sum_{i=1}^{n}x_{i}^{2} }$ the Euclidean norm.
## SOM updating
After finding the #BMU, units in the #SOM are updated. During this **update procedure**, the #BMU **is updated to be a little closer to the sample vector in the input space.**

The topological neighbors of the #BMU are also similarly updated. This **update procedure stretches the #BMU and its topological neighbors towards the sample vector**.

The computational effort consists of finding a #BMU among all the neurons and updating the weight vectors in the neighborhood of the winner unit.
- If the neighborhood is large, there are a lot of weight vectors to be updated. This is the case in the beginning of the training process, where it is recommended to use large neighborhoods
- In the case of large networks, relatively larger portion of the time is spent looking for a winner neuron.

Each update an be thought of as a *shift in the topology of a neighborhood* local to the output point.

With sufficiently many iterations through this process, the nodes will form the dimension reduced feature space over the original input which best preserves the local topology.![](Pasted%20image%2020240507142137.png)
The #SOM **update rule** for any unit $m_{i}$:$$m_{i}(t+1)=m_{i}(t)+h_{ci}(t)\cdot[x(t)-m_{i}(t)]$$where $t$ denotes time, since this is a training process through time.
- The $x(t)$ is the input vector drawn from the input data set at time $t$
- $h_{ci}(t)$ is a non-increasing #neighborhood-function around the winner unit $m_{c}$.
	- When $h_{ci}(t)=1$, $m_{i}(t+1)=m_{i}(t)+ [x(t)-m_{i}(t)]=x(t)$. Update to be the input vector
	- When $h_{ci}(t)=0$, $m_{i}(t+1)=m_{i}(t)$, no update.
## Neighborhood function
The #neighborhood-function includes two parts:
- A **decreasing learning rate function** $\alpha(t)$
- A **function** dictating the **form of the neighborhood function**, which also determines the **rate of change around the winner unit** $m_{c}$.

It can be written as:$$h_{ci}(t)=\alpha(t)\cdot \exp(-\frac{||r_{i}-r_{c}||^{2}}{2\sigma(t)^{2}})$$in the case of the Gaussian neighborhood function around the winner neuron $m_{c}$.
- $r_{i}$ - position of the excited neuron $i$
- $r_{c}$ - position of the winning neuron
- $\sigma(t)$ decreases over time.

A #neighborhood-function with a **Gaussian kernel** around the winner neuron is computationally demanding as the exponential function has to be calculated, but can well be approximated by the "bubble" neighborhood function. (a)

The **bubble** #neighborhood-function is a constant function. Every neuron in the neighborhood is updated the same proportion of the difference between the neuron and the presented sample vector. (b)![](Pasted%20image%2020240507144046.png)The bubble neighborhood function is a **good compromise between the computational cost and the approximation of the Gaussian**.

## BMU's neighborhood
The neighborhood decreases over time. The figure is drawn assuming the neighborhood remains centered on the same node, in practice the #BMU will move around according to the input vector being presented to the network. 

Over time the neighbourhood will shrink to the size of just one node, i.e., the BMU itself.![](Pasted%20image%2020240507152856.png)
## Learning rate
The #learning-rate $\alpha(t)$ is a **decreasing function of time $t$**. Three functions are common:
1. A **linear function** decreases to zero linearly during the learning from its initial value (a)$$\alpha(t,T)=\alpha(0)\cdot \frac{1-t}{T}$$with $T$ the total number of iterations #epochs, $t$ a specific iteration.
2. An **inverse function** decreases rapidly from the initial value (b)$$\alpha(t)=\alpha(0)\cdot 1/t$$
3. A **power function**$$\alpha(t,T)=\alpha(0)\cdot e^{-\frac{t}{T}}$$
The initial values for $\alpha(0)$ must be determined. Usually when using a rapidly decreasing inverse $\alpha$ function, the initial values can be larger than in the linear case.
![](Pasted%20image%2020240507153549.png)
The learning is usually *performed in two phases*:
1. First round relatively large initial alpha values are used $\alpha=0.3,\dots,0.99$
2. Small initial $\alpha$ values $\alpha=0.1,\dots,0.01$ are used during the other round.
## Cooperation
When the #BMU is found, its **own weights and its neighbor neuron's weights are updated together** using the weight update formula, which returns updated weights for the next iteration.![](Pasted%20image%2020240507153649.png)
## Adaptation
There are **two hyper-parameters** controlling the learning speed of the #SOM network, including the **learning rate** and the **neighbor size**.

The #adaptation process aims to **decrease these two hyper-parameters** after each iteration to make the #SOM **learning converged**.

The #adaptation of the parameters is **done once for each iteration/epoch**.

## Comments on SOM training!
There are a number of alternative equations used in the learning process of self-organizing maps. 

A lot of research tries to get the optimal values for the number of iterations, the learning rate, and the neighborhood radius. 

The inventor, Teuvo Kohonen, suggested that this learning process should be split into two phases.
- **Phase 1 Learning**: The learning rate is reduced from 0.9 to 0.1 and the neighborhood radius from half the diameter of the lattice to the immediately surrounding nodes. 
- **Phase 2 Fine-Tuning**: The learning rate is further reduced from 0.1 to 0.0. However, there would be double or more iterations in Phase 2 and the neighborhood radius value should remain fixed at 1, meaning the BMU only.
# 9.3 Visualisation using U-matrix
A #U-matrix **Unified distance matrix** representation of #SOM visualizes the distance between the neurons, also called distance map.

The distance between the adjacent neurons is calculated and presented with different colorings between the adjacent nodes.
- A **dark coloring** between the neurons corresponds to ***a large distance and thus a gap between the weight values in the input space***. 
- A **light coloring** between the neurons signifies that ***the weight vectors are close to each other in the input space***.

**Light areas** can be thought as *clusters* and **dark areas** as *cluster separators*. 
	This can be a helpful presentation when one tries to find clusters in the input data without having any a prior information about the clusters.

Teaching #SOM and representing it with the #U-matrix offers a fast way to get insight of the data distribution.
![](Pasted%20image%2020240507154827.png)
The neurons of the network are marked as black dots. The representation reveals that these are a separate cluster in the upper right corner of this representation. The clusters are separated by a dark gap
## Tuto: How to build it
Read that, j'ai la flemme:
![IL2233VT24_Lecture_9_slides](IL2233VT24_Lecture_9_slides.pdf#page=29)
## U-matrix
The #U-matrix contains in **each cell the euclidean distance** (in the input space) **between neighboring cells**.
- *Small values* mean that *SOM nodes are close together* in the input space, whereas *larger values* mean that *SOM nodes are far apart*, ***even if they are close in the output space***. 
- Usually, those distance values are discretised, color-coded based on intensity and displayed as a kind of heatmap. 
- When such distances are depicted in a gray-scale image, light colors depict closely spaced node weight vectors and darker colors indicate more widely separated node weight vectors. Thus, groups of light colors can be considered as clusters, and the dark parts as the boundaries between the clusters![](Pasted%20image%2020240507155103.png)
## U-matrix with variation
Instead of hexagonal units, #U-matrix may come in a 2D topology with **rectangular units**. The ***idea is the same*** ( to calculate the distance and visualize a color map according to the distance values) but the *exact distance calculation might be different depending on the implementation*.
![](Pasted%20image%2020240507155250.png)The red box can be calculated by averaging the four neighbor units in yellow color.

Implementation in code:
	![](Pasted%20image%2020240507155418.png)
# 9.4 Application
![IL2233VT24_Lecture_9_slides](IL2233VT24_Lecture_9_slides.pdf#page=35)

# 9.5 Summary
![IL2233VT24_Lecture_9_slides](IL2233VT24_Lecture_9_slides.pdf#page=45)