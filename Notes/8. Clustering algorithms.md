[IL2233VT24_Lecture_8_slides](IL2233VT24_Lecture_8_slides.pdf)
# Supervised vs Unsupervised learning
#supervised-learning given a set of $p$ features $X_{1},X_{2}, \dots, X_{p}$ measured on $n$ observations, and a response $Y$ also measured on those same $n$ observations, the goal is then to predict $Y$ using $X_{1},X_{2},\dots ,X_{p}$.
- **Classification** for **discrete** responses
- **Regression** for **continuous** responses such as time series, real values.

#Unsupervised-learning given a set of $p$ features $X_{1}, X_{2},\dots ,X_{p}$ measured on $n$ observations.
- We are not interested in prediction, because we do not have an associated response variable $Y$
- #clustering: **a broad class of methods for discovering unknown subgroups in data**.

#Clustering refers to a very broad set of techniques for finding subgroups, clusters in a data set. We seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other.
$\to$ #clustering is #classification without labels. [Scikit functions](IL2233VT24_Lecture_8_slides.pdf#page=6).
# K-means clustering
#K-means #clustering: we seek to **partition** the observations into a **pre-specified number $K$ of clusters**, which are **distinct and non-overlapping**.![](Pasted%20image%2020240506154152.png)
The idea behind #K-means clustering is that a good clustering $C_{k}$ is one for which the *within-cluster variation* $W$ is as small as possible.$$\min_{C_{1},C_{2},\dots,C_{k}}\sum_{k=1}^{K}W(C_{k})$$$W$ measures the amount by which the observations within a cluster differ from each other.

One most common choice involves squared Euclidean distance.
![IL2233VT24_Lecture_8_slides](IL2233VT24_Lecture_8_slides.pdf#page=10)
# Hierarchical clustering
In #hierarchical #clustering, we do **not know in advance how many clusters** we want; in fact, we end up with a tree-like visual representation of the observations, called a *dendrogram*, that allows us to view at once the clusterings obtained for each possible number of clusters, from $1$ to $n$.

![IL2233VT24_Lecture_8_slides](IL2233VT24_Lecture_8_slides.pdf#page=17)
# Dynamic Time Warping (DTW)
![IL2233VT24_Lecture_8_slides](IL2233VT24_Lecture_8_slides.pdf#page=37)
The

# Examples


# Hierarchical clustering
To encounter the disadvantage of #K-means clustering that it requires to pre-specify the number of clusters $K$.
- #hierarchical #clustering results in an attractive tree-based representation of the observations called #dendrogram
## Agglomerative clustering
By bottom up or agglomerative clustering, a #dendrogram  is built starting from the leaves and combining clusters up to the trunk.
- Each leaf of the #dendrogram represents one of the observations. Moving up the tree, some leaves begin to fuse into branches corresponding to observations that are similar to each other
- For any of two observations, we can **look in the tree where branches containing those two observations are first fused**. The **height** of this fusion (measured on the vertical axis) indicates how different they are.
- Observations that fuse at the very bottom of the tree are quite similar to each other. And thus the one at the **top of the tree** will **tend to be quite different**
![](Pasted%20image%2020241111112347.png)

## Dendrogram based clustering
To identify clusters on the basis of a #dendrogram, we make a horizontal cut across the dendrogram. The distinct sets of observations beneath the cut can be interpreted as clusters.![](Pasted%20image%2020241111112631.png)
## Hierarchical clustering algorithm
1. Starting out at the bottom of the #dendrogram, each of the $n$ observations is treated as its own cluster. The 2 clusters most similar to each other are then fused so that there are now $n-1$ clusters
2. Next the 2 clusters most similar to each other are fused again so that there now are $n-2$ clusters
3. The algo proceeds in this fashion until all of the observations belong to one single cluster, and the dendrogram is complete.

The algorithm proceeds #iteratively:
1. Begin with $n$ observations and a measure #Euclidean-distance or #DTW of all the pairwise dissimilarities. Treat each observation as its own cluster
2. For $i=1,2,\dots,n$:
	a. Examine all pairwise inter-cluster dissimilarities among the $i$ clusters and identify the pair of clusters that are least dissimilar or most similar
	b. Fuse these two clusters, the dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed
	c. Compute the new pairwise inter-cluster dissimilarities among the $i-1$ remaining clusters
![](Pasted%20image%2020241111113137.png)

Now that we have the points at the correct size, #linkage needs to be made because we have a concept of the dissimilarity between pairs of observations, but how do we define the dissimilarity between two clusters if one or both of the clusters contains multiple observations?
- This extension is achieved by developing the notion of #linkage, which **defines the dissimilarity between two groups of observations**.![](Pasted%20image%2020241111113504.png)
The #dendrogram is deeply impacted by the type of linkage chosen:![](Pasted%20image%2020241111113546.png)
### Dissimilarity measures
#Euclidean-distance $d(\vec{x}, \vec{y})=\sqrt{ \sum_{i=1}^{n}(x_{i}-y_{i})^{2} }$ $\implies$ look at the **magnitude** of the series
#Correlation-based-distance considers two observations to be **similar if their features are highly correlated** even though the observed values may be far apart in terms of Euclidean distance.  $\implies$ look at the **profile** of the series![](Pasted%20image%2020241111114022.png)
### Other issues
#Data-scaling: Consider whether or not the variables should be scaled to have standard deviation one before the dissimilarity between the observations is computed.

#Soft-clustering: Since #K-means and #hierarchical clustering force every observation into a cluster, the *clusters found may be heavily distorted* due to the presence of **outliers** that do not belong to any cluster. Mixture models are an attractive approach for accommodating the presence of such outliers. These amount to a soft version of K-means clustering.

#Robustness: Clustering methods generally are **not very robust to perturbations** to the data. For instance, suppose that we cluster $n$ observations, and then cluster the observations again after *removing a subset* of the $n$ observations at random. One would hope that the two sets of clusters obtained would be quite similar, but often this is not the case!
# Dynamic Time Warping #DTW
#Euclidean-distance and variants have issues:
- only compares time series of same length
- very sensitive with respect to 6 signal transformations: **shifting, uniform, amplitude scaling, uniform time scaling, uniform bi-scaling, time warping and non-uniform amplitude scaling**
- Does not handle outliers or noise
![](Pasted%20image%2020241111151333.png)
	C. S. Perng, H. Wang, S. R. Zhang and D. S. Parker, ”Landmarks: a new model for similarity-based pattern querying in time series databases,” 16th International Conference on Data Engineering, 2000, pp. 33-42.

#DTW gives more #robustness to the similarity computation
- does not do uniform point-to-point pairing but allows many-to-one and one-to-many pairing when computing distance
- Allows to compare signals of different length and recognize similar shapes (even if they present signal transformation such as shifting or scaling)
![](Pasted%20image%2020241111151620.png)
Given two time series $T=\{t_{1},\dots,t_{n}\}$ and $S=\{ s_{1},\dots,s_{m} \}$, we can define *distMatrix* (see above) where $distMatrix(i,j)=d(T_{i},S_{j})$ is the #Euclidean-distance of $i$-th point of $T$ and $j$-th point of $S$ for $i \in [1,n]$ and $j\in[1,m]$.

The goal of #DTW is to find the #warping-path $W=\{ w_{1},w_{2},\dots,w_{k}\dots,w_{K} \}$ of contiguous elements in *distMatrix* with:
- $\max(n,m)<K<m+n-1$
- $w_{k}=distMatrix(i,j)$ such that it minimizes the function:$$DTW(T,S)=\min\left( \sqrt{ \sum_{k=1}^{K}w_{k} } \right)$$

We have thus the following constraints given $w_{k}=(i,j)$ and $w_{k-1}=(i',j')$ with $i,i' \leq n$ and $j,j'\leq m$:
- **Boundary conditions** $w_{1}=(1,1)$ and $w_{K}=(n,m)$
- **Continuity** $i-i'\leq 1$ and $j-j'\leq 1$
- **Monotonicity** $i-i'\geq 0$ and $j-j'\geq 0$

The #warping-path can be efficiently computed using dynamic programming:
- A cumulative distance matrix $\gamma$ of the same dimension as the *distMatrix* is created to store in the cell $(i,j)$ the following value$$\gamma(i,j)=d(T_{i},S_{j})+min(\gamma(i-1,j-1),\gamma(i,j-1),\gamma(i-1,j))$$
- The overall complexity is relative to the computation of all distances in *distMatrix* and is thus $O(nm)$.
- The last element of the warping path $w_{K}$ corresponds to the distance calculated with the #DTW method.

![](Pasted%20image%2020241111152704.png)
## Example
![](Pasted%20image%2020241111153742.png)

Another example is for the series: $x = \{ 3,1,2,2,1 \}$ and $y=\{ 2,0,0,3,3,1,0 \}$
![](Pasted%20image%2020241111153852.png)
In python: package *fastdtw*:
- pip install fastdtw
![](Pasted%20image%2020241111154320.png)
![](Pasted%20image%2020241111154334.png)
### DTW for alignments
Can also be used for alignments.
## References
- Understanding Dynamic Time Warping. https://databricks.com/blog/2019/ 04/30/understanding-dynamic-time-warping.html 
- Programatically understanding dynamic time warping (DTW). https://nipunbatra.github.io/blog/ml/2014/05/01/dtw.html#Visualizing-the-distance-matrix 
- An Illustrative Introduction to Dynamic Time Warping. https://towardsdatascience.com/an-illustrative-introduction-to-dynamic-time-warping-36aa98513b98
- DTW function in Matlab https://www.mathworks.com/help/signal/ref/dtw.html 
- Cassisi, Carmelo & Montalto, Placido & Aliotta, Marco & Cannata, Andrea & Pulvirenti, Alfredo. (2012). ”Similarity Measures and Dimensionality Reduction Techniques for Time Series Data Mining”. In Chapter 3 of book ”Advances in Data Mining Knowledge Discovery and Applications”.