[IL2233VT24_Lecture_8_slides](IL2233VT24_Lecture_8_slides.pdf)
# Supervised vs Unsupervised learning
#supervised-learning given a set of $p$ features $X_{1},X_{2}, \dots, X_{p}$ measured on $n$ observations, and a response $Y$ also measured on those same $n$ observations, the goal is then to predict $Y$ using $X_{1},X_{2},\dots ,X_{p}$.
- **Classification** for **discrete** responses
- **Regression** for **continuous** responses such as time series, real values.

#Unsupervised-learning given a set of $p$ features $X_{1}, X_{2},\dots ,X_{p}$ measured on $n$ observations.
- We are not interested in prediction, because we do not have an associated response variable $Y$
- #clustering: **a broad class of methods for discovering unknown subgroups in data**.

#Clustering refers to a very broad set of techniques for finding subgroups, clusters in a data set. We seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other.
$\to$ #clustering is #classification without labels. [Scikit functions](IL2233VT24_Lecture_8_slides.pdf#page=6).
# K-means clustering
#K-means #clustering: we seek to **partition** the observations into a **pre-specified number $K$ of clusters**, which are **distinct and non-overlapping**.![](Pasted%20image%2020240506154152.png)
The idea behind #K-means clustering is that a good clustering $C_{k}$ is one for which the *within-cluster variation* $W$ is as small as possible.$$\min_{C_{1},C_{2},\dots,C_{k}}\sum_{k=1}^{K}W(C_{k})$$$W$ measures the amount by which the observations within a cluster differ from each other.

One most common choice involves squared Euclidean distance.
![IL2233VT24_Lecture_8_slides](IL2233VT24_Lecture_8_slides.pdf#page=10)
# Hierarchical clustering
In #hierarchical #clustering, we do **not know in advance how many clusters** we want; in fact, we end up with a tree-like visual representation of the observations, called a *dendrogram*, that allows us to view at once the clusterings obtained for each possible number of clusters, from $1$ to $n$.

![IL2233VT24_Lecture_8_slides](IL2233VT24_Lecture_8_slides.pdf#page=17)
# Dynamic Time Warping (DTW)
![IL2233VT24_Lecture_8_slides](IL2233VT24_Lecture_8_slides.pdf#page=37)
# Examples