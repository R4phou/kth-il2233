[IL2233VT24_Lecture_7_slides](IL2233VT24_Lecture_7_slides.pdf)
# 1. Why RNN?
**Feedforward** #NN:
- **Representation strength**: can use wide or deep layers to model arbitrarily complex non-linear functions between input vector and output vector
- **Representation weakness**: memory-less architecture (*Output = F(input)*), a fixed-size input vector is mapped to a fixed-size output vector
- **Application limitation**: can be difficult to deal with sequence based applications such as language translation, voice recognition, text-to-voice interpretation, object tracking, etc.
	- Input/output with possibly variable length
	- Have to be aware of context (past and future data)

**Recurrent Neural Network** #RNN answers some of these limitations.
# 2. Vanilla RNN
The basics of #RNN is to **introduce memory/state into** #NN thanks to a **delayed feedback**, *current output goes back to the input at the next time step*.

A #NN or fully connected #NN layer $A$ (in square box), takes in some input $X_{t}$ and outputs a value/state $h_{t}$. A loop allows the state to be passed from one time step of network $A$ to the next.![](Pasted%20image%2020240417145317.png)Unrolling the loop gives a view as a normal #NN architecture in a chain structure (right in the picture). 

By unfolding, the *network is expanded in the time domain for the complete sequence*.
	**An example** **in machine translation:** If the sequence is a sentence of 4 words, the network operates as if being unrolled into a 4-layer neural network, one layer for each word.

**A key point** to gain some space/time, **re-use the same NN cell with the same set of weight/biases at all time steps**. Unlike a traditional deep NN, which uses different parameters at each layer, an #RNN **shares the same parameters across all time steps**.

The same function $f$ is applied to transit the system state in different time steps$$h^{t}=f(h^{t-1},x^{t};\theta)$$
The #RNN usually has a **maximum length of a sequence**. 
- In theory,  RNN supports a sequence of arbitrary length. In practice, this is problematic.
- *If* the sequence is *too long*, it can *lead to the gradient vanishing or explosion problem*.

Here we have the difference between #RNN vs #MLP:![](Pasted%20image%2020240417150145.png)As we can see, the RNN allows to have less input? #toverif

## RNN inference
The input is a concatenation of the current input $(t)$ and the previous $(t-1)$ state.. The first input $(t_{0})$ uses the initial state.

#RNN may generate output at each time instant. This means that the loss function shall be the sum of losses for all or partial times, depending on the application.![](Pasted%20image%2020240417150414.png)
## RNN with sequences
![](Pasted%20image%2020240417150508.png)

## Word/Character predictor
**Task:** Given a few characters or words as input, predict the next character or word.![](Pasted%20image%2020240417152252.png)
Given a sequence of previous characters, build an #RNN to predict the probability distribution of the next character in the sequence.
![IL2233VT24_Lecture_7_slides](IL2233VT24_Lecture_7_slides.pdf#page=12)

#Many-to-one application: sentiment analysis, musical genre analysis, bird song identification etc.

#One-to-many application: from image to text...

#Many-to-many application: voice recognition, machine translation

The **sequence-to-sequence** is handled by an encoder-decoder architecture. A summary vector $C$ is generated in between.![](Pasted%20image%2020240417162411.png)
# 3. Long Short Term Memory network #LSTM
**Short range** dependency **works well** in vanilla #RNN 
	Predict the last word in a sentence: "Fish is in *water*". Short range context is sufficient!

**Long range** dependency:
	‚ÄùHe was born in China. He has done many great things. . . . . He speaks *Chinese*. 

It is possible but **difficult** **to train** a vanilla #RNN **to capture** #long-term **dependency** due to the gradient vanishing/explosion (Multiplying a number of fractional numbers results in nearly 0).

**Gradient vanishing/exploding** is a common *problem for deep neural networks*, but **worse for RNNs** due to *sharing the same weights* (in time).
## LSTM
**Long Short Term Memory networks** #LSTM are special kinds of #RNN capable of learning #long-term dependencies much better than RNNs.

It does so by introducing gated control of input, current state and output.![](Pasted%20image%2020240417162925.png)
#LSTM **also have the chain-like structure**, but the **repeating module** (often called cell) has a **different** structure. Instead of having a single **neural network layer**, there are **four**, interacting in a very special way.
### LSTM cell
At each time step, #LSTM has **3 inputs**: $X_{t}, h_{t-1}, C_{t-1}$ and **two outputs:** $h_{t}, C_{t}$.

The state on the **upper line** $C_{t}$ represents **long-term memory**, while the state on the lower line $h_{t}$ records working memory or short-term memory.
### Long-term memory representation and control
The #LSTM cell has a cell state for long time effect. It runs down the entire chain, with only some minor linear interactions.
![](Pasted%20image%2020240418134658.png)
**Gates control** adding or removing information to the cell state:
- A gate comprises a #sigmoid $\sigma$ network layer and a point-wise multiplication operation
- The sigmoid layer outputs numbers between 0 and 1, determining how much of each component should be let through.
	0 : Let nothing through
	1 : Let everything through
## Stages
### Forget gate control
**Stage 1**: The #forget-gate-layer decides **what information to throw away** from the cell state![](Pasted%20image%2020240418134830.png)
### Input gate control
**Stage 2**: The #input-gate-layer decides **what new information to store** in the cell state.
1. It first decides which values to update, and a tanh layer creates a vector of new candidate values, $\tilde{C_{t}}$, that could be added to the state
2. Then combine these two to *create* an *update to the state*.![](Pasted%20image%2020240418135229.png)
### Output gate control
**Stage 3**: The #output-gate-layer decides what to output, based on the cell state, but a filtered version.
1. Run the sigmoid gate layer deciding what parts of the cell state to output
2. The cell state goes through tanh (to push the values to be between -1 and 1) and multiply it by the output of the sigmoid gate
![](Pasted%20image%2020240418135414.png)
## Information flow controlled by gates
The cell state is implicit, which can be reset, kept, or modified at each time step.

The cell state further controls the hidden state generation at each time step.
![](Pasted%20image%2020240418140035.png)
# 4. Various RNNs
## Deep RNN
Each hidden state is continuously passed to both the next time step of the current layer and the current time step of the next layer.![](Pasted%20image%2020240418140234.png)
## Bi-directional RNN
Often we need information about both past and future to determine the present:
	Fill a missing word (not necessarily the last one) in a sentence. "He is *Happy* because he wins the competition."![](Pasted%20image%2020240418140308.png)
## Recursive Neural Network #RvNN
**Recursive Neural Network** #RvNN applies the same set of weights recursively over a structured input, to produce output. 

It can operate on any *hierarchical tree structure*, parsing through input nodes, combining child nodes into parent nodes and combining them with other child/parent nodes to **create a tree-like structure**.![](Pasted%20image%2020240418141822.png)

#RNN do the same, but the structure is strictly linear (weights are applied on the first input, then the second, third and so on).
## NN with explicit memory
From implicit memory to **explicit memory**:
- Some knowledge can be stored explicitly and fetched for use when needed, rather than computed each and every time when needed
- Enable storage intelligence besides computational intelligence. ![](Pasted%20image%2020240418141928.png)
# 5. Examples of RNN computation and sentence completion
![IL2233VT24_Lecture_7_slides](IL2233VT24_Lecture_7_slides.pdf#page=37)