[IL2233VT24_Lecture_4_slides](IL2233VT24_Lecture_4_slides.pdf)
# 1. Autoregressive (AR) models
#AR **Autoregressive** model or process:![](Pasted%20image%2020240412131847.png)We can also see some **AR(p)** model, with $p$ the order of the model:
	![](Pasted%20image%2020240412131949.png)

For example the **AR(1)** model:$$y_{t}=c+\phi_{1}y_{t-1}+\epsilon_{t}$$If $\phi_{1}=$
- 0, $y_{t}$ is equivalent to #WN 
- 1 and $c=0$, $y_{t}$ is equivalent to a #RW 
- 1 and $c \neq 0$, $y_{t}$ is equivalent to a #RW with drift
- $< 0$, $y_{t}$ tends to oscillate between positive and negative values.
## Stationarity
To determine whether a series is #stationary we can use this **stationary condition**:
	We normally restrict autoregressive #AR models to stationary data and then some constraints on the values of the parameters are required.![](Pasted%20image%2020240412132759.png)Note that estimation software takes care of this.

We can also see that there is a difference between **strictly** and **weakly** #stationary process
- **Strictly stationary process**: the joint distribution of a finite sub-sequence of random variables of the stochastic process remains the same as we shift it along the time axis, i.e., shift invariance.
	- Difficult to obtain and process its joint distribution
- **Weakly stationary series** is *more used*, which is *described by its characteristic statistic* using low-order moments such as mean, variance.

A #weakly-stationary series $\{ Y_{t} \}$ satisfies the following three conditions:
1. $\forall t \in T$, second-order moment $EY_{t}^{2} <\infty$
2. $\forall t \in T$, first-order moment $EY_{t}=\mu$, $\mu$ is a constant
3. $\forall s,t \in T$ and $t+\Delta, s+\Delta \in T$, co-variance $cov(t,s)=cov(t+\Delta,s+\Delta)$
$\to$ Weakly stationary series **means second-order moment stationary**.

From the $3^{rd}$ condition, we can derive that the series has a constant variance, $DY_{t}=cov(t,t)=cov(0,0), \forall t \in T$.

But it is important to know that **not all #AR processes are #stationary**: the coefficients of an AR process determine whether the AR process is stationary or not.
	![](Pasted%20image%2020240412134029.png)
# 2. Moving Average (MA) models
**Moving Average** #MA models:![](Pasted%20image%2020240412134049.png)
Different order illustration:![](Pasted%20image%2020240412134440.png)It is possible to write any #stationary **AR(p) process as an MA(infinite)** process:
	![](Pasted%20image%2020240412134828.png)
One really cool thing about #MA is #invertibility:
- Any **MA(q)** process can be written as an **AR(inf)** process if we impose some constraints on the MA parameters. Then the MA model is called “invertible”.
- Invertible models have some mathematical properties that make them easier to use in practice. 
- **Invertibility is the counterpart to stationarity** for the MA part of an ARMA process. 
- Invertibility of an ARIMA model is equivalent to forecastability of an ETS (Exponential Smoothing) model.![](Pasted%20image%2020240412135012.png)
Once more, estimation software takes care of this.

But **not all #MA process are invertible**:
- *The coefficients of a MA process determine whether the MA process is invertible* or not. 
- *Different MA(q) processes may generate the same ACF graphs*. 
- MA #Invertibility sets **constraints on the coefficients**, ensuring a **one-to-one mapping from ACF graph to its corresponding MA model**.
# ARMA models
#ARMA **Autoregressive Moving Average models**
- Predictors include *both lagged values of $y_{t}$ and lagged errors*
- Conditions on coefficients ensure #stationarity and #invertibility.
![](Pasted%20image%2020240412140148.png)
# 3. Differencing and Backshift notation B
#differencing is used to transform non-stationary series into #stationary series (see chap 3).

The differenced series is the change between each observation in the original series:i$$y_{t}'=y_{t}-y_{t-1}$$It has thus $T-1$ values since it cannot calculate a difference $y_{1}'$ for the first observation.

For **first order differencing** see #RW.

Occasionally, we need to difference the data a second time: **second-order differencing**:$$y_{t}''=y_{t}'-y_{t-1}'=(y_{t}-y_{t-1})-(y_{t-1}-y_{t-2})=y_{t}-2y_{t-1} 
+ y_{t-2}$$$y_{t-2}$ has $T-2$ values.

We almost never go beyond second-order differences.

We can also talk about **seasonal differencing**, it is the difference between an observation and the corresponding observation from the previous season$$y_{t}'=y_{t}-y_{t-m}$$where $m$ is the length of a season.
- For seasonal series, seasonally differenced series is closer to being stationary.
- Remaining non-stationarity can be removed with further first difference
	![](Pasted%20image%2020240412143524.png)

**If seasonality is strong**, we **recommend that seasonal differencing be done first** because sometimes the resulting series will be stationary and there will be no need for further first difference.

When both seasonal and first differences are applied, it makes no difference which is done first—the result will be the same.

The #backshift-notation is a very useful notation $B$$$By_{t}=y_{t-1}$$In other words, $B$, operating on $y_{t}$ , has the effect of shifting the data back one period. Two applications of $B$ to $y_{t}$ shifts the data back two periods.$$B(By_{t})=B^{2}y_{t}=y_{t-2}$$And thus for **monthly data**, we use $B^{12}$.
- **first-order differencing** $y_{t}' = y_{t}-By_{t}=(1-B)y_{t}$
- **second-order differencing** $y_{t}'' = y_{t}-2By_{t}+ B^{2}y_{t}=(1-B)^{2}y_{t}$
![](Pasted%20image%2020240412144001.png)

# 4. ARIMA
#ARIMA or **Autoregressive Integrated Moving Average models** is based on #ARMA
![](Pasted%20image%2020240412144240.png)![](Pasted%20image%2020240412144258.png)
![IL2233VT24_Lecture_4_slides](IL2233VT24_Lecture_4_slides.pdf#page=30)
# 5. Seasonal ARIMA models
![IL2233VT24_Lecture_4_slides](IL2233VT24_Lecture_4_slides.pdf#page=33)