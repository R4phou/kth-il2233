Based on the Wold Decomposition theorem, a stationary process can be approximated by an #ARMA model. In practice, finding that approximation may not be easy.

#Box-Jenkins popularized a method that combines the moving average and the autoregressive approaches.

Note that building #ARIMA models generally requires more experience than commonly used statistical methods such as regression.
[IL2233VT24_Lecture_5_slides](IL2233VT24_Lecture_5_slides.pdf)

# The Box-Jenkins method
There are **3 primary stages in building** a #Box-Jenkins time series model and **1 stage for using it for prediction**.
1. Model identification
2. Parameter estimation and Model selection
3. Model Validation
4. Model Prediction

In order for it to be effective, it generally requires at least a moderately long series.
## 1. Model identification
It consists in **determining** if the series is #stationary and if there is a significant #seasonality that needs to be modeled.
- #stationarity can be assessed from a run sequence or #line-plot, the run sequence plot should show constant location and scale. Otherwise it can be detected with #ACF, specifically, non-stationarity is often indicated by an #autocorrelation plot with very slow decay
- #seasonality or periodicity can usually be assessed from an autocorrelation plot, a seasonal sub series plot or a spectral plot.
### Achieve stationarity
To achieve #stationarity, #Box-Jenkins recommend #differencing. However, fitting a curve and subtracting the fitted values from the original data can also be used in the context of Box-Jenkins models. But there are **3 techniques to reach** #stationarity:
- **Difference the data**: given a series $y_{t}$, we create a new series $y_{t}'=y_{t}-y_{t-1}$. 
	The differenced data will contain one less point than the original data. Although you can difference the data more than once, one difference is often sufficient.
- If the data contain a #trend, we can **fit some type of curve to the data** and then **model the residuals from that fit**.
	Since the purpose of the fit is to simply remove long term trend, a simple fit such as a straight line is typically used.
- **For non-constant variance**, taking the **logarithm root of the series may stabilize the variance**. #variance-stabilisation
	For negative data, you can add a suitable constant to make all the data positive before applying the transformation. This constant can then be subtracted from the model to obtain predicted values and forecasts for future points.

If the data show different variation at different level of the series, a *transformation can be useful*:
	Assume the original series $\{ y_{1},y_{2},\dots,y_{n} \}$, generate a transformed series $\{ z_{1},z_{2},\dots, z_{n} \}$ where:
	- Square root transform $z_{t}=\sqrt{ y_{t} }$
	- Logarithm transform (more useful because they are more interpretable, changes are relative on the original scale) $z_{t}=\log y_{t}$
##### Box-Cox transformation
Each of these transformation is close to a member of the family of #Box-Cox transformation:$$z_t = \begin{cases} \log(y_t), & \text{if } \lambda = 0 \\ \left( \frac{y_{\lambda t} - 1}{\lambda} \right), & \text{if } \lambda \neq 0 \end{cases}$$Depending on the value of $\lambda$:
- $\lambda=1$, no substantive transformation.
- $\lambda=1/2$, square root plus linear transformation.
- $\lambda=0$, natural logarithm transformation.
- $\lambda=-1$, negative reverse plus 1.
	If some $y_{t}=0$, then we must have $\lambda\neq 0$.
	If some $y_{t} < 0$, to use logarithm, add a constant to make all $y_{t}$ positive
##### Back transformation
We must reverse the transformation to obtain the forecasts on the original scale, the #back-transformation is basically the **reverse** #Box-Cox:$$y_t = \begin{cases} 
e^{z_t}, & \text{if } \lambda = 0 \\
\left( \frac{(\lambda z_t + 1)}{\lambda} \right), & \text{if } \lambda \neq 0 
\end{cases}
$$
And for the #differencing operation, the back transformation is called an #integration step done by$$y_{t}=y_{t}'+y_{t-1}$$
### Seasonality
Many time series display seasonality, which means periodic fluctuations. If seasonality or periodicity is present, it must be incorporated into the time series model. But we have **two issues** to **detect** and **model** #seasonality.

To **detect** #seasonality with **visualisation** (see examples below):
- A #line-plot will often show it
- Multiple box can be an alternative. It *assumes that the seasonal period is known.*
- If the period is not known, #ACF plot can help identify seasonality
	If there is significant seasonality, the *autocorrelation plot should show spikes at lags equal to the period*. 
![IL2233VT24_Lecture_5_slides](IL2233VT24_Lecture_5_slides.pdf#page=14)
## Order identification
Once the #stationarity and #seasonality are addressed, the next step is to **identify** the #order $p$ and $q$ of the **autoregressive and moving average terms** #AR and #MA terms.
### Order p of AR process
Since the #PACF of an $AR(p)$ process becomes $0$ at #lag $p+1$, we examine the sample PACF to see where it becomes $0$.

This can be done by placing a 95% confidence interval on the sample PACF plot.
	![](Pasted%20image%2020240414121158.png)
For an $AR(1)$ process, the sample autocorrelation function should have an exponentially decreasing appearance.

However, higher order #AR processes are often a mixture of exponentially decreasing and damped sinusoidal components, thus #ACF **is generally not helpful for identifying the order of the AR process**.
[Example](IL2233VT24_Lecture_5_slides.pdf#page=23).
### Order q of MA process
Since the #ACF of an $MA(q)$ process becomes $0$ at #lag $q+1$ and greater, we examine the sample ACF to see where it essentially becomes $0$.

This can be done by placing the 95% confidence interval for the sample ACF on the sample autocorrelation plot.

The sample #PACF is **generally not helpful for identifying the order of the #MA process**.
[Example](IL2233VT24_Lecture_5_slides.pdf#page=27).


To conclude this first step:
**The theoretical ACF and PACF for the AR, MA, and ARMA models are known, and are different for each model**. These differences among models are important to select models.![](Pasted%20image%2020240414122145.png)
## 2.1 Parameter estimation
Once the model order has been identified, we need to estimate the parameters ($c, \phi_{1}, \dots, \phi_{p}$ and $\theta_{1},\dots,\theta_{q}$ and $\sigma^{2}$).

The main approach for that are **non-linear squares** and **maximum-likelihood estimation** #MLE.

**Estimating the parameters** is a **complicated non-linear estimation problem**. For this reason, the parameter estimation should *be left to a high quality software program*. Different software may give slightly different answers as they use different methods of estimation, and different optimisation algorithms.
## 2.2 Model selection - information criterion
Often, there are many models which can fit the data. To determine which one to select we often use the **Akaike's Information Criterion** #AIC, which weights both model accuracy and model simplicity$$AIC = -2\log L+2(p+q+k+1)$$where $L$ is the likelihood of the data, 
- $k=1$ if intercept $c\neq 0$
- $k=0$ if $c=0$
- The second term is the number of parameters in the model (including $\sigma^{2}$, the variance of the residuals).

The principle of selection is so called **parsimony**, which is **in favor of a simpler model**: ***A model with a smaller AIC value is better***. 

We have different versions of it:
- The **Corrected AIC** #AICc$$AICc = AIC +\frac{2(p+q+k+1)(p+q+k+2)}{T-p-q-k-2}$$
- The **Bayesian Information Criterion** #BIC$$BIC=AIC + [\log T-2](p+q+k-1)$$
Good models are obtained by **minimizing** either #AIC, #AICc or #BIC.
## 3. Model diagnostics/validation
An assumption for a stable univariate process is that *the error term $\epsilon_{t}$ is assumed to follow the assumptions for a stationary univariate process*. The **residuals should be white noise** #WN (or independent when their distributions are normal) drawings from a fixed distribution with a constant mean and variance.

If the #Box-Jenkins model is a good model for the data, the **residuals should be white noise**. **Otherwise**, we go back to the model identification step and try to **develop a better model**. Hopefully the *analysis of the residuals can provide some clues as to a more appropriate model*.

### Portmanteau test
The #portmanteau test is a test for #time-series to check whether any of a group of autocorrelations of the residual time series are different from zero.

Instead of testing randomness at every distinct lag, **it** **tests the "overall" randomness based on a number of lags**, and is therefore called the portmanteau test.

Consider a whole set of autocorrelation $r_{k}$ values, check if the whole set is significantly different from a zero set.
- #Box-Pierce test$$Q=T\sum_{k=1}^{h}\gamma_{k}^{2}$$where $h$ is the max lag considered, and $T$ number of observations. 
	If each $\gamma_{k}$ is zero or small, $Q$ is zero or small. 
	If some $\gamma_{k}$ values are large, $Q$ is large.
- #Ljung-Box test$$Q=T(T+2)\sum_{k=1}^{h}(T-k)^{-1}\gamma_{k}^{2}$$Preferences: $h=10$ for non-seasonal data, $h=2m$ for seasonal data. It has **better performance**, especially in small samples.
## 4. Multi-step forecasts
1. Rearrange #ARIMA equation so $y_{t}$ is on left hand side #LHS
2. Rewrite equation by replacing $t$ by $T+h$
3. On #RHS, replace future observations by their forecasts, future errors by zero and past errors by corresponding residuals
![IL2233VT24_Lecture_5_slides](IL2233VT24_Lecture_5_slides.pdf#page=38)
### Prediction intervals
![](Pasted%20image%2020240414144956.png)
- Prediction intervals increase in size with forecast horizon. 
- Prediction intervals can be difficult to calculate by hand. 
- Calculations assume residuals are uncorrelated and normally distributed. 
- Prediction intervals tend to be too narrow
	- the uncertainty in the parameter estimates has not been accounted for. 
	- the ARIMA model assumes historical patterns will not change during the forecast period. 
	- the ARIMA model assumes uncorrelated future errors
# Training and test
SEE ML course.
# Examples
![IL2233VT24_Lecture_5_slides](IL2233VT24_Lecture_5_slides.pdf#page=48).