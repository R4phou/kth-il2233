[IL2233VT24_Lecture_3_slides](IL2233VT24_Lecture_3_slides.pdf)

This section concerns #feature-extraction, #stationary and #decomposition.

# 1. Case study: Investigate a medical time-series signal
#EEG **Electroencephalography** is a medical monitoring technique that records an electrogram of the electrical activity on the scalp representing the macroscopic activity of the brain. It works by measuring the brain voltage activity with high temporal resolution (milliseconds) but poor spatial resolution (10cm²).

It give us back a #time-series signal. Indeed as an ordered data sequence, a #time-series **may be treated as a digitalised signal**. And thus basic signal processing methods are applicable!
![](Pasted%20image%2020240411110309.png)
This frequency bands are widely used for #EEG:![](Pasted%20image%2020240411110846.png) 
https: //towardsdatascience.com/time-series-signals-the-fourier-transform-f68e8a97c1c2
# 2. What feature to discover from data
There are  3 features that characterise the collected measurement data:
- #statistical-feature: #mean, #variance, ...
- #temporal-feature: #ACF, #PACF, ...
- #spectral-feature : #power-spectrum, #spectrogram, ...
## Signal sampling
Let's define:
- $\Delta$ - the sampling interval: interval between samples (1ms here)
- $N$ - the total number of points observed
- $T = N \Delta$ - the total time of the recording/sampling
- The sampling frequency $f_{0}=1/\Delta$ (=1000Hz here)
- $x$ - symbol for the data
- $x_{n}$ - explicitly indicate the index $n\in\{ 1,2,3,\dots,N \}$ to the sample number.

We can now look if there is a **dominant rhythmic activity** in the data, and look at its frequency. Let's look at this data:![](Pasted%20image%2020240411111521.png)We count there 6 peaks/troughs in the first 0.1s corresponding to approximately 60 peaks/troughs in 1s. Thus as there are 60 cycles per second, we have a frequency of 60Hz.

Since the sampling rate is 1000Hz, one cycle has $1000/60=16,6667$ data points spanning $\implies 1/60=0.01667s$.

We can also see that the alternative current in North American electrical grids alternates at 60Hz. Hence, the data is **dominated by electrical noise**. We should then **look if there is another rhythmic activity on the background**.
## Statistical features
If $n$ is a time instant, then $x$ is a time series. #statistical-feature
- Mean $\bar{x}=\sum_{n=1}^{N}x_{n}$
- Variance $\sigma^{2}=\frac{1}{N}\sum_{n=1}^{N}(x_{n}-\bar{x})^{2}$
## Temporal feature
#temporal-feature is #auto-covariance. It is often used to assess the dependent structure in the data.

The #auto-covariance $r_{xx}[L]$ evaluated at #lag $L$ is defined as$$r_{xx}[L]=\frac{1}{N}\sum_{n=1}^{N-L}(x_{n+L}-\bar{x})(x_{n}-\bar{x})$$
This is a **biased estimate** of the true auto-covariance. To compute the **unbiased estimate of the covariance**:$$r_{xx}[L]=\frac{1}{N-L}\sum_{n=1}^{N-L}(x_{n+L}-\bar{x})(x_{n}-\bar{x})$$
[Look here for illustrations and examples](IL2233VT24_Lecture_3_slides.pdf#page=13).

We can notice that the #auto-covariance of the #EEG signal is periodic as #EEG signal is itself periodic.

When plotting the #ACF of the #EEG signal, we can see that the dominant 60Hz activity manifests as periodic peaks and troughs in the autocovariance function.![](Pasted%20image%2020240411141847.png)Remember that #ACF is auto-covariance normalised with auto-covariance $r_{xx}[0]$.
## Spectral feature
The #power-spectrum indicates the **amplitude** of **rhythmic activity** in $x$ as a function of the **frequency**.
- The power spectrum of  signal $x$ is the magnitude squared to the #DFT of $x$

The  **power spectral density** #psd or #spectrum describes the extent to which sinusoïds of a single frequency capture the structure of the data.
	To compute the power over any range of frequencies: integrate the spectrum over that frequency range.

For more information about [Discrete Fourier Transform](IL2233VT24_Lecture_3_slides.pdf#page=20) #DFT $X_{j}$.

The #power-spectrum is $S_{xx,j}=\frac{2\Delta^{2}}{T}X_{j} \cdot X_{j}^{*}$
	$2\Delta^{2}/T$ is simply a numerical scaling
	The unit is in this case $(\mu V)^{2}/$Hz
![](Pasted%20image%2020240411142921.png)
The x-axis are just the indices. But to convert indices to frequencies we use:
- the **frequency resolution**, $df=1/T$, or the reciprocal of the total recording duration
- the **Nyquist frequency** (maximum observable frequency), $f_{NQ}=f_{0}/2=1/(2\Delta)$ or *half of the sampling frequency*.
	Here as $T=2$, $df=1/2=0.5$Hz
	And as $f_{0}=1000$Hz, $f_{NQ}=1000/2$Hz $=500$Hz

The spectrum is dominated by a single peak at 60 Hz. **Other weaker rhythmic activity might be present but invisible**, because the large 60 Hz peak saturates the vertical scale. Therefore, we put it to the **logarithmic scale** to obtain **decibels** and **uncover the weaker rhythmic activity**.

To change to the decibel scale, we first divide the spectrum by the maximum value observed (at the dominant 60 Hz) and then take the logarithm base 10 of this ratio and multiply the result by 10.
	In our example we can see that two peaks have emerged at frequencies 5-15Hz.
![](Pasted%20image%2020240411205502.png)

Similarly, we can use the logarithmic scale to stretch the low-frequency part of the horizontal axis. 
	The two low-frequency peaks become more apparent.
![](Pasted%20image%2020240411205632.png)
### Spectrogram
The #spectrogram allows us to break the time series into smaller intervals of data and compute the spectrum for each interval.
- This allows to see that some weaker signals apply only for some periods and not all periods
- This intervals can be quite small and overlap?
- The result is the spectrum as a function of frequency and time. It provides insight into the spectral features that change in time.
![](Pasted%20image%2020240411205842.png)
	Here we can see that the #EEG data are dominated by 60Hz noise
	However weaker low-frequency activity emerges during two intervals:
	- A 6Hz rhythm in the $\theta$ band from 0s to 1s
	- An 11Hz rhythm in the $\alpha$ from 1s to 2s.
# 3. Time-Series stationarity and decomposition
## Stationarity
Classical time series analysis method, #ARIMA **Auto-Regression Integrated Moving-Average** assumes that the series is #stationary.

If $y_{t}$ is a #stationary series, then for all $s$, the joint distribution of $(y_{t},y_{t+1},\dots,y_{t+s})$ is **dependent on** $s$ but **independent on $t$**. It also has a **constant mean $\mu$ and variance $\sigma^{2}$** .

It can be checked with visual inspection of the #line-plot or with *statistic quantity check* such as the  #ADF test. [ADF test information](IL2233VT24_Lecture_3_slides.pdf#page=35).
### Techniques to reach stationary
For example, the #RW (random walk) series is a non-stationary series. To turn it into a stationary series, we use **first order** #differencing (first and second order differencing below).![](Pasted%20image%2020240411211153.png)The order needed can be determined thanks to statistical tests, we need to **avoid under-differencing AS WELL AS over-differencing**.
## Time-series decomposition
A time series is often structured, patterned.
- #Trend: a pattern showing there is a long-term increase or decrease in the data. 
- #Seasonal: a pattern exists when a series is influenced by a seasonal factor (e.g. a quarter, a month, or each day of a week.), or more generally, a fixed period. 
- #Cyclic: a pattern exists when data exhibit rise and fall at not-a-fixed period (duration usually at least 2 years for chronological data)

The difference between #Seasonal and #Cyclic can be checked from:
- The **period**:
	- For seasonal pattern we have **constant length**
	- For cyclic pattern we have **variable length**
- **Short-Long term**: average length of a cycle is longer than that of a season
- **Magnitude**: the magnitude of a cycle is more variable than that of a season
- **Predictability**: Generally, the timing of peaks and troughs is predictable with seasonal data, but unpredictable in the long term with cyclic data
### General decomposition
#general-decomposition looks like this:
General decomposition
- $y_t$ is data at time t.
- $T_t$ = trend-cycle component at time t
- $S_t$ = seasonal component at time t
- $R_t$ = remainder component at time t$$y_t = f(S_t + T_t + R_t)$$
**Classical decomposition** is:
- additive $Y[t]=T[t]+S[t]+e[t]$
- multiplicative $Y[t]=T[t]*S[t]*e[t]$

Note that the log() operation turns a multiplicative relationship into an additive relationship.

For negative data, one can add a suitable constant to make all the data positive before applying the transformation. This constant can then be subtracted from the model to obtain predicted (i.e., the fitted) values and forecasts for future data points.

**STL** “Seasonal and Trend decomposition using Loess” (Loess: Local regression) is: #STL
- Very versatile and robust to outliers.
- STL allows seasonal component to change over time, and rate of change controlled by user.
- Smoothness of trend-cycle also controlled by user.
- Only additive.
- Take logs to get multiplicative decomposition.
- Use Box-Cox transformations to get other decompositions.

[Example](IL2233VT24_Lecture_3_slides.pdf#page=44).
